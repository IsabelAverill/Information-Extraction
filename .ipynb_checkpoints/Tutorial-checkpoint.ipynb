{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We'll be using the [fake news dataset](https://www.kaggle.com/mrisdal/fake-news) from kaggle for this notebook. The dataset contains text and metadata scraped from 244 websites tagged as \"bullshit\" by the [BS Detector](https://github.com/selfagency/bs-detector) Chrome Extension by [Daniel Sieradski](https://github.com/selfagency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_fake = pd.read_csv('fake.csv')\n",
    "df_fake = df_fake[['title', 'text', 'language']]\n",
    "df_fake = df_fake.loc[(pd.notnull(df_fake.title)) & (pd.notnull(df_fake.text)) & (df_fake.language == 'english')]\n",
    "df_fake = df_fake.reset_index(drop=True)\n",
    "df_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process\n",
    "\n",
    "This is one of the most important step in analyzing the text data. If the preprocessing is not good, the algorithm can't do much since we are feeding it a lot of noise, in other words, **Garbage In Garbage Out**. So let's first clean our data using the following techniques:\n",
    "\n",
    "1. Tokenization\n",
    "2. Stopword removal\n",
    "3. Strip punctuation\n",
    "4. Lemmatization\n",
    "5. Bigram collocation detection. (Bigrams are sets of two adjacent tokens. Collocations are frequently co-occurring tokens. Using bigrams, phrases like \"machine_learning\" can be discovered in our output which otherwise would have been treated as two separate words \"machine\" and \"learning\". Spaces are replaced with underscores in corpus)\n",
    "\n",
    "Note: Stemming and lemmatization are two different processes for reducing the morphological variation of words. \n",
    "- Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.\n",
    "\n",
    "- Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\n",
    "\n",
    "    The word \"better\" has \"good\" as its lemma. This link is missed by stemming, as it requires a dictionary look-up.\n",
    "\n",
    "    The word \"walk\" is the base form for word \"walking\", and hence this is matched in both stemming and lemmatisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet') # download wordnet to be used in lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess(texts):\n",
    "    # tokenization\n",
    "    texts = [re.findall(r'\\w+', line.lower()) for line in texts]\n",
    "    # remove stopwords\n",
    "    texts = [remove_stopwords(' '.join(line)).split() for line in texts]\n",
    "    # remove punctuation\n",
    "    texts = [strip_punctuation(' '.join(line)).split() for line in texts]\n",
    "    # remove words that are only 1-2 character\n",
    "    texts = [[token for token in line if len(token) > 2] for line in texts]\n",
    "    # remove numbers\n",
    "    texts = [[token for token in line if not token.isnumeric()] for line in texts]\n",
    "    # lemmatization \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    texts = [[word for word in lemmatizer.lemmatize(' '.join(line), pos='v').split()] for line in texts]\n",
    "    \n",
    "    return texts\n",
    "\n",
    "# pre-processing\n",
    "processed_texts = preprocess(df_fake.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_number = 0\n",
    "processed_texts[doc_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "# training for bigram collocation detection\n",
    "phrases = Phrases(processed_texts, min_count=1, threshold=0.8, scoring='npmi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a performant Phraser object to transform any tokenized sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram[['white', 'house', 'million', 'money']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging detected collocations with data\n",
    "processed_texts = list(bigram[processed_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# create dictionary mappings for training data\n",
    "dictionary = Dictionary(processed_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now remove rare words and common words based on their document frequency to further prevent noisy results. Below we remove words that appear in less than 10 documents or in more than 60% of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out words that occur less than 10 documents, or more than 60% of the documents.\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we transform the documents to a vectorized form. We simply compute the frequency of each word, including the bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in processed_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our final corpus looks like in vectorized form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_number = 0\n",
    "corpus[doc_number]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A document is represented as a list of tuples of (vocab ID, frequency) for each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "Now that we have our corpus and dictionary, we train the topic model using gensim's api for LDA algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "# Only necessary if the model isn't in the Git, which it is.\n",
    "# training LDA model\n",
    "# model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=35, passes=50 , chunksize=1500, iterations=200, alpha='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only necessary if the model isn't in the Git, which it is.\n",
    "# model.save('lda_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LdaModel.load('models/lda_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll see what the typical output of a topic model looks like. Below, we print top 5 topics. As we can see, each topic is associated with a set of words, and each word has a probability of being expressed under that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.show_topics(num_topics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake.loc[24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other APIs that can be used to explore the trained topic model, for getting further information about our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `get_term_topics` returns the odds of a particular word belonging to some particular topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_term_topics('money')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_document_topics` method returns topic distribution of the document along with topic distribution for each word in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_number = 0\n",
    "doc_topic, word_topic, phi_value = model.get_document_topics(corpus[doc_number], per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output gives the topic distribution of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_fake.loc[6])\n",
    "print(dictionary[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output gives a ranked list of topics for every word. The lower the rank, more is the probability of the word  belonging to that topic in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phi values are essentially the probability of that word in that document belonging to a particular topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "## Manual:\n",
    "\n",
    "We would like to know if the correct thing has been learned, does the topics inferred make sense as per our text data.\n",
    "Thanks to pyLDAvis, we can visualise our topic models in a really handy way and inspect what words the topics consist of or how similar the topics are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(model, corpus, dictionary, sort_topics=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what topics does the fake news primarily focuses on, and according to our dataset its mostly about politics. \n",
    "\n",
    "#### Left Panel:\n",
    "\n",
    "1. The area of the circles is proportional to the prevalence of the topics in corpus. So, we can visually determine about the most important topics in our corpus.\n",
    "2. The positioning of the topics is done according to their inter-topic distances, which to some extent preserves the semantic similarity allowing some related topics to form clusters, it is however a little difficult to determine exactly how similar the topics are. For this, we can directly visualize the matrix of inter-topic distances and know the exact distance (with intersecting/different words) between any pair of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "from plotly.graph_objs import *\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "py.init_notebook_mode()\n",
    "\n",
    "def plot_difference(mdiff, title=\"\", annotation=None):\n",
    "    \"\"\"\n",
    "    Helper function to plot difference between models\n",
    "    \"\"\"\n",
    "    annotation_html = None\n",
    "    if annotation is not None:\n",
    "        annotation_html = [[\"+++ {}<br>--- {}\".format(\", \".join(int_tokens), \", \".join(diff_tokens))\n",
    "                            for (int_tokens, diff_tokens) in row]\n",
    "                           for row in annotation]\n",
    "        \n",
    "    data = Heatmap(z=mdiff, colorscale='RdBu', text=annotation_html)\n",
    "    layout = Layout(width=950, height=950, title=title,\n",
    "                       xaxis=dict(title=\"topic\"), yaxis=dict(title=\"topic\"))\n",
    "    py.iplot(dict(data=[data], layout=layout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_matrix, annotation = model.diff(model, distance='jensen_shannon', num_words=50)\n",
    "plot_difference(difference_matrix, title=\"Topic difference [jensen shannon distance]\", annotation=annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2D coordinates which are inferred for each topic in the left panel of pyldavis are based on above distance matrix. [Principal coordinate analysis (PCoA)](https://mb3is.megx.net/gustame/dissimilarity-based-methods/principal-coordinates-analysis) is used for infering the 2d coordinates which seeks to preserve the original topic distances of higher dimensions. You can refer to [this](http://occamstypewriter.org/boboh/2012/01/17/pca_and_pcoa_explained/) blog for getting a graphical reasoning behind this technique.\n",
    "\n",
    "\n",
    "#### Right Panel:\n",
    "\n",
    "1. The terms are initially ranked according to their saliency, when no topic is selected. Saliency basically depends on frequency of the term in corpus, and how informative the specific term w is for determining the individual topics.  For example, if a word w occurs in all topics, observing the word tells us little about the document’s topical mixture <sup>[1]</sup>.\n",
    "2. The tuning parameter, 0≤λ≤1, controls how the terms are ranked for each selected topic, with terms listed in decreasing order of relevance. The relevance of term w to topic t is defined as λ*p(w∣t)+(1−λ)*p(w∣t)/p(w). Values of λ near 1 give high relevance rankings to frequent terms within a given topic, whereas values of λ near zero give high relevance rankings to exclusive terms within a topic i.e. it factors in the general probability of that word in other topics.\n",
    "\n",
    "But how does this ranking relate to the ranking of terms in gensim's `show_topic()` method that we saw above. In gensim, the float value given, next to every term as shown in the list output of `show_topic()` is the value of p(w∣t) i.e. probability of term w for topic t, and gensim simply ranks the topic terms according to this probability value. So, in order to have similar results as gensim’s `show_topic()` in pyLDAvis, λ can be set to 1 which will then result in the relevance being directly proportional to only p(w∣t). \n",
    "\n",
    "Now if we choose λ very close to 1, common terms of the corpus appear near the top for multiple topics, making it hard to differentiate between the meanings of different topics. And if choose λ very close to 0, it can still remain noisy, by giving high rankings to very rare terms that occur in only a single topic, for instance. While such terms may contain useful topic content, but if they are very rare the topic may remain difficult to interpret.\n",
    "\n",
    "The optimal value suggested for λ is 0.6<sup>[2]</sup>.\n",
    "\n",
    "\n",
    "\n",
    "## Automatic:\n",
    "\n",
    "[Coherence](http://qpleple.com/topic-coherence-to-evaluate-topic-models/) is often used to get past the manual inspection and objectively compare the topic models. By returning a score, we can compare between different topic models of the same corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "CoherenceModel(model, texts=processed_texts, dictionary=dictionary, window_size=10).get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is a really nice explanation for understanding the intution behind coherence that I came across in Matti Lyra's [Pydata Berlin talk](https://github.com/mattilyra/pydataberlin-2017/blob/master/notebook/EvaluatingUnsupervisedModels.ipynb):\n",
    "\n",
    "Take the following document that talk about ice hockey. I've highlighted terms that I think are related to the subject matter, you may disagree with my judgement. Notice that among the terms that I've highlighted as being part of the topic of Ice Hockey are words such as Penguin, opposing and shots. None of these on the face of it would appear to belong to Ice Hockey, but seeing them in context makes it clear that Penguin refers to the ice hockey team, shots refers to disk shaped pieces of vulcanised rubber being launched at the goal at various different speeds and opposing refers to the opposing team.\n",
    "\n",
    "> Rinne stopped 27 of 28 **shots** from the **Penguins** in **Game** 6 at home Sunday, but that lone **goal** allowed was enough for the **opposition** to break out the **Stanley Cup trophy** for the second straight **season**.\n",
    "\n",
    "Given the terms that I've determined to be a partial description of Ice Hockey (the concept), one could conceivably measure the coherence of that concept by counting how many times those terms occur with each other i.e. co-occur in some sufficiently large reference corpus.\n",
    "\n",
    "One of course encounters a problem should the reference corpus never refer to ice hockey. A poorly selected reference corpus could for instance be patent applications from the 1800s, it would be unlikely to find those word pairs in that text.\n",
    "\n",
    "This is precisely what several research papers have aimed to do. To take the top words from the topics in a topic model and measure the support for those words forming a coherent concept / topic by looking at the co-occurrences of those terms in a reference corpus. The research up to now was finally wrapped up into a single paper where the authors develop a coherence pipeline, which allows plugging in all the different methods into a single framework. This coherence pipeline is partially implemented in gensim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application\n",
    "\n",
    "In this section, we will go through some of the applications or ways in which we can use topic models and get the most out of them for our NLP tasks.\n",
    "\n",
    "\n",
    "## Document clustering\n",
    "\n",
    "Using the `get_document_topics`, we can visualize the documents based on their topic distribution. That topic distribution representation of each document can be used for clustering the semantically similar documents together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get document topics\n",
    "all_topics = model.get_document_topics(corpus, minimum_probability=0)\n",
    "all_topics[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output shows the topic distribution of first document in the corpus as a list of (topic_id, topic_probability).\n",
    "\n",
    "Now, using the topic distribution of a document as it's vector representation, we will plot all the documents in our corpus using Tensorboard.\n",
    "\n",
    "### Prepare the Input files for Tensorboard\n",
    "\n",
    "Tensorboard takes two input files, one containing the embedding vectors and the other containing relevant metadata. As described above, we will use the topic distribution of documents as their representative vector and the metadata file will consist of Document titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create file for tensors(vectors)\n",
    "with open('doc_lda_tensor.tsv','w') as w:\n",
    "    for doc_topics in all_topics:\n",
    "        for topics in doc_topics:\n",
    "            w.write(str(topics[1])+ \"\\t\")\n",
    "        w.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create file for metadata(documet titles)\n",
    "with open('doc_lda_metadata.tsv','w') as w:\n",
    "    for doc_id in range(len(all_topics)):\n",
    "        w.write(df_fake.title[doc_id] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go to http://projector.tensorflow.org/ and upload these two files by clicking on Load data in the left panel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Connections\n",
    "\n",
    "Quite often, the sheer size of text data is highly unorganized which makes it difficult to browse or search through the corpus. For instance, a scholar in a narrow subdiscipline, searching for a particular research article, would certainly be interested to learn that the topic of that article is highly correlated with another topic that the researcher may not have known about, and that is not explicitly contained in the article. Alerted to the existence of this new related topic, the researcher could browse the collection in a topic-guided manner to begin to investigate connections to a previously unrecognized body of work. Topic connections can be extremely useful especially for the datasets with interdisciplinary interests.\n",
    "\n",
    "### Topic Network\n",
    "\n",
    "Networks can be a great way to explore topic models. We can use it to navigate that how topics belonging to one context may relate to some topics in other context and discover common factors between them. We can use them to find communities of similar topics and pinpoint the most influential topic that has large no. of connections or perform any number of other workflows designed for network analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get topic distributions\n",
    "topic_dist = model.state.get_lambda()\n",
    "\n",
    "# get topic terms\n",
    "num_words = 50\n",
    "topic_terms = [{w for (w, _) in model.show_topic(topic, topn=num_words)} for topic in range(topic_dist.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, a distance matrix is calculated to store distance between every topic pair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "from gensim.matutils import jensen_shannon\n",
    "import networkx as nx\n",
    "import itertools as itt\n",
    "\n",
    "# calculate distance matrix using the input distance metric\n",
    "def distance(X, dist_metric):\n",
    "    return squareform(pdist(X, lambda u, v: dist_metric(u, v)))\n",
    "\n",
    "topic_distance = distance(topic_dist, jensen_shannon)\n",
    "topic_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The edges will be created based on the distance between topic pair. Next, we'd have to define a threshold of distance value such that the topic-pairs with distance above that does not get connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# store edges b/w every topic pair along with their distance\n",
    "edges = [(i, j, {'weight': topic_distance[i, j]})\n",
    "         for i, j in itt.combinations(range(topic_dist.shape[0]), 2)]\n",
    "\n",
    "# keep edges with distance below the threshold value\n",
    "k = np.percentile(np.array([e[2]['weight'] for e in edges]), 20)\n",
    "edges = [e for e in edges if e[2]['weight'] < k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nodes of the network graph will represent topics and the edges represent distance between two connecting nodes/topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add nodes and edges to graph layout\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(range(topic_dist.shape[0]))\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "graph_pos = nx.spring_layout(G)\n",
    "\n",
    "# initialize traces for drawing nodes and edges \n",
    "node_trace = Scatter(\n",
    "    x=[],\n",
    "    y=[],\n",
    "    text=[],\n",
    "    mode='markers',\n",
    "    hoverinfo='text',\n",
    "    marker=Marker(\n",
    "        showscale=True,\n",
    "        colorscale='YIGnBu',\n",
    "        reversescale=True,\n",
    "        color=[],\n",
    "        size=10,\n",
    "        colorbar=dict(\n",
    "            thickness=15,\n",
    "            xanchor='left'\n",
    "        ),\n",
    "        line=dict(width=2)))\n",
    "\n",
    "edge_trace = Scatter(\n",
    "    x=[],\n",
    "    y=[],\n",
    "    text=[],\n",
    "    line=Line(width=0.5, color='#888'),\n",
    "    hoverinfo='text',\n",
    "    mode='lines')\n",
    "\n",
    "\n",
    "# no. of terms to display in annotation\n",
    "n_ann_terms = 10\n",
    "\n",
    "# add edge trace with annotations\n",
    "for edge in G.edges():\n",
    "    x0, y0 = graph_pos[edge[0]]\n",
    "    x1, y1 = graph_pos[edge[1]]\n",
    "    \n",
    "    pos_tokens = topic_terms[edge[0]] & topic_terms[edge[1]]\n",
    "    neg_tokens = topic_terms[edge[0]].symmetric_difference(topic_terms[edge[1]])\n",
    "    pos_tokens = list(pos_tokens)[:min(len(pos_tokens), n_ann_terms)]\n",
    "    neg_tokens = list(neg_tokens)[:min(len(neg_tokens), n_ann_terms)]\n",
    "    annotation = \"<br>\".join((\": \".join((\"+++\", str(pos_tokens))), \": \".join((\"---\", str(neg_tokens)))))\n",
    "    \n",
    "    x_trace = list(np.linspace(x0, x1, 10))\n",
    "    y_trace = list(np.linspace(y0, y1, 10))\n",
    "    text_annotation = [annotation] * 10\n",
    "    x_trace.append(None)\n",
    "    y_trace.append(None)\n",
    "    text_annotation.append(None)\n",
    "    \n",
    "    edge_trace['x'] += x_trace\n",
    "    edge_trace['y'] += y_trace\n",
    "    edge_trace['text'] += text_annotation\n",
    "\n",
    "# add node trace with annotations\n",
    "for node in G.nodes():\n",
    "    x, y = graph_pos[node]\n",
    "    node_trace['x'].append(x)\n",
    "    node_trace['y'].append(y)\n",
    "    node_info = ''.join((str(node+1), ': ', str(list(topic_terms[node])[:n_ann_terms])))\n",
    "    node_trace['text'].append(node_info)\n",
    "    \n",
    "# color node according to no. of connections\n",
    "for node, adjacencies in enumerate(nx.generate_adjlist(G)):\n",
    "    node_trace['marker']['color'].append(len(adjacencies))\n",
    "    \n",
    "fig = Figure(data=Data([edge_trace, node_trace]),\n",
    "             layout=Layout(showlegend=False,\n",
    "                hovermode='closest',\n",
    "                xaxis=XAxis(showgrid=True, zeroline=False, showticklabels=True),\n",
    "                yaxis=YAxis(showgrid=True, zeroline=False, showticklabels=True)))\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On hovering over the nodes, we'll see the topic_id along with it's top words and on hovering over the edges, we'll see the intersecting/different words of the two topics that the edge connects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the above graph, we used the 20th percentile of all the distance values as our threshold. But you can experiment with few different values also such that the graph doesn’t become too crowded or too sparse and we could get an optimum amount of information about similar topics or any interesting relations between different topics.\n",
    "\n",
    "### Topic dendrogram\n",
    "\n",
    "The topics can be related to each other in a hierarchical form also. For ex. in case of a research paper corpora, we could have papers belonging to maths, physics, biology which can further be categorised into sub-groups. Maths papers can be sub-divided into topics such as Calculus, Algebra, Geometry; Physics papers into mechanics, electronics, astronomy; Biology into Genetics, Anatomy, Molecular etc.\n",
    "\n",
    "Dendrogram is a tree-structured graph which can be used to visualize the result of a hierarchical clustering. We can use it to explore the topic models and see how the topics are connected to each other in a sequence of successive fusions or divisions that occur in the clustering process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial as scs\n",
    "from scipy.cluster import hierarchy as sch\n",
    "\n",
    "# get topic distributions\n",
    "topic_dist = model.state.get_lambda()\n",
    "\n",
    "# get topic terms\n",
    "num_words = 300\n",
    "topic_terms = [{w for (w, _) in model.show_topic(topic, topn=num_words)} for topic in range(topic_dist.shape[0])]\n",
    "\n",
    "# no. of terms to display in annotation\n",
    "n_ann_terms = 10\n",
    "\n",
    "# use Jensen-Shannon distance metric in dendrogram\n",
    "def js_dist(X):\n",
    "    return pdist(X, lambda u, v: jensen_shannon(u, v))\n",
    "\n",
    "# define method for distance calculation in clusters\n",
    "linkagefun=lambda x: sch.linkage(x, 'single')\n",
    "\n",
    "# calculate text annotations\n",
    "def text_annotation(topic_dist, topic_terms, n_ann_terms, linkagefun):\n",
    "    # get dendrogram hierarchy data\n",
    "    linkagefun = lambda x: sch.linkage(x, 'single')\n",
    "    d = js_dist(topic_dist)\n",
    "    Z = linkagefun(d)\n",
    "    P = sch.dendrogram(Z, orientation=\"bottom\", no_plot=True)\n",
    "\n",
    "    # store topic no.(leaves) corresponding to the x-ticks in dendrogram\n",
    "    x_ticks = np.arange(5, len(P['leaves']) * 10 + 5, 10)\n",
    "    x_topic = dict(zip(P['leaves'], x_ticks))\n",
    "\n",
    "    # store {topic no.:topic terms}\n",
    "    topic_vals = dict()\n",
    "    for key, val in x_topic.items():\n",
    "        topic_vals[val] = (topic_terms[key], topic_terms[key])\n",
    "\n",
    "    text_annotations = []\n",
    "    # loop through every trace (scatter plot) in dendrogram\n",
    "    for trace in P['icoord']:\n",
    "        fst_topic = topic_vals[trace[0]]\n",
    "        scnd_topic = topic_vals[trace[2]]\n",
    "        \n",
    "        # annotation for two ends of current trace\n",
    "        pos_tokens_t1 = list(fst_topic[0])[:min(len(fst_topic[0]), n_ann_terms)]\n",
    "        neg_tokens_t1 = list(fst_topic[1])[:min(len(fst_topic[1]), n_ann_terms)]\n",
    "\n",
    "        pos_tokens_t4 = list(scnd_topic[0])[:min(len(scnd_topic[0]), n_ann_terms)]\n",
    "        neg_tokens_t4 = list(scnd_topic[1])[:min(len(scnd_topic[1]), n_ann_terms)]\n",
    "\n",
    "        t1 = \"<br>\".join((\": \".join((\"+++\", str(pos_tokens_t1))), \": \".join((\"---\", str(neg_tokens_t1)))))\n",
    "        t2 = t3 = ()\n",
    "        t4 = \"<br>\".join((\": \".join((\"+++\", str(pos_tokens_t4))), \": \".join((\"---\", str(neg_tokens_t4)))))\n",
    "\n",
    "        # show topic terms in leaves\n",
    "        if trace[0] in x_ticks:\n",
    "            t1 = str(list(topic_vals[trace[0]][0])[:n_ann_terms])\n",
    "        if trace[2] in x_ticks:\n",
    "            t4 = str(list(topic_vals[trace[2]][0])[:n_ann_terms])\n",
    "\n",
    "        text_annotations.append([t1, t2, t3, t4])\n",
    "\n",
    "        # calculate intersecting/diff for upper level\n",
    "        intersecting = fst_topic[0] & scnd_topic[0]\n",
    "        different = fst_topic[0].symmetric_difference(scnd_topic[0])\n",
    "\n",
    "        center = (trace[0] + trace[2]) / 2\n",
    "        topic_vals[center] = (intersecting, different)\n",
    "\n",
    "        # remove trace value after it is annotated\n",
    "        topic_vals.pop(trace[0], None)\n",
    "        topic_vals.pop(trace[2], None)  \n",
    "        \n",
    "    return text_annotations\n",
    "\n",
    "# get text annotations\n",
    "annotation = text_annotation(topic_dist, topic_terms, n_ann_terms, linkagefun)\n",
    "\n",
    "# Plot dendrogram\n",
    "dendro = ff.create_dendrogram(topic_dist, distfun=js_dist, labels=range(1, 36), linkagefun=linkagefun, hovertext=annotation)\n",
    "dendro['layout'].update({'width': 1000, 'height': 600})\n",
    "py.iplot(dendro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The x-axis or the leaves of hierarchy represent the topics of our LDA model, y-axis is a measure of closeness of either individual topics or their cluster. Essentially, the y-axis level at which the branches merge (relative to the \"root\" of the tree) is proportional to their similarity.\n",
    "\n",
    "Text annotations visible on hovering over the merging nodes show the intersecting/different terms of it's two child nodes. Merging node on first hierarchy level uses the topics on leaves directly, to calculate intersecting/different terms, and the upper nodes assume the intersection(+++) as the topic terms of it's child node.\n",
    "\n",
    "This type of tree graph could help us see the high level cluster theme that might exist in our data, as we can see the common/different terms of combined topics in a cluster head annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Coloring\n",
    "\n",
    "We can explore the topic distribution of documents in a visually handy way by coloring its words according to topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "    \n",
    "def color_words(model, doc):\n",
    "    # make into bag of words\n",
    "    doc = model.id2word.doc2bow(doc)\n",
    "    # get word_topics\n",
    "    doc_topics, word_topics, phi_values = model.get_document_topics(doc, per_word_topics=True)\n",
    "    \n",
    "    # color-topic matching\n",
    "    ys = [i*2 for i in range(len(doc_topics))]\n",
    "    topic_colors = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "    \n",
    "    # set up fig to plot\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "    # a sort of hack to make sure the words are well spaced out.\n",
    "    word_pos = 1/len(doc)\n",
    "    \n",
    "    # use matplotlib to plot words\n",
    "    for word, topics in word_topics:\n",
    "        ax.text(word_pos, 0.8, model.id2word[word],\n",
    "                horizontalalignment='center',\n",
    "                verticalalignment='center',\n",
    "                fontsize=20, color=topic_colors[topics[0]],  # choose just the most likely topic\n",
    "                transform=ax.transAxes)\n",
    "        word_pos += 0.2 # to move the word for the next iter\n",
    "\n",
    "    ax.set_axis_off()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_words(model, processed_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. http://vis.stanford.edu/files/2012-Termite-AVI.pdf\n",
    "2. https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:info-ex]",
   "language": "python",
   "name": "conda-env-info-ex-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
